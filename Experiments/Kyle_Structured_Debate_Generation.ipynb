{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fpgUZzErarU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Goal: to improve the performance of a small LLM in generating competitive debate speeches through prompt engineering and architecture at inference time alone\n",
        "\n",
        "Rationale: Large SOTA closed-source LLMs like Gemini are good at generating emotionally charged arguments for debates but their debate speeches themselves could use improvement, and the size of these models makes them unwieldy. We want to see if we can get smaller open-source LLMs to match their performance without requiring expensive training"
      ],
      "metadata": {
        "id": "8w-gArDf6tuO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiments in architecture modifications:\n",
        "\n",
        "We add a special layer for argument extraction and refutation in hopes that this will improve the way our responses are structured. The refutations will be passed into the response generator as context"
      ],
      "metadata": {
        "id": "n3bg7sbmLkRg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After 12/3, we may try prompt engineering for each layer to find the best-working prompt"
      ],
      "metadata": {
        "id": "dNEe75cbmsaS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "0weqcV_0d0Vu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some of this might not be necessary. Copied from HW1"
      ],
      "metadata": {
        "id": "v59SeO09d3Oe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U gdown\n",
        "!gdown https://drive.google.com/uc?id=1wUpcOYU-eT1pe_je6qQLa8v4TBPUhPae\n",
        "!unzip code.zip\n",
        "!rm  -r __MACOSX\n",
        "!mv cs224v_hw1_code/* .\n",
        "!rm -r cs224v_hw1_code\n",
        "!rm requirements.txt\n",
        "!rm notebook.ipynb"
      ],
      "metadata": {
        "id": "W--M7jaQd6nU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dspy-ai==3.0.3\n",
        "!pip install crawl4ai==0.7.4\n",
        "!pip install langchain_text_splitters==0.3.11\n",
        "!pip install jupyter==1.0.0\n",
        "!pip install nbconvert[webpdf]\n",
        "\n",
        "!playwright install"
      ],
      "metadata": {
        "id": "VWiD7dNwd7i6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from typing import List, Tuple\n",
        "\n",
        "import dspy\n",
        "import httpx\n",
        "from tqdm import tqdm\n",
        "\n",
        "from src.dataclass import RetrievedDocument, LiteratureSearchAgentResponse, LiteratureSearchAgentRequest\n",
        "from src.encoder import Encoder\n",
        "from src.literature_search import LiteratureSearchAgent\n",
        "from src.lm import init_lm, LanguageModelProviderConfig, LanguageModelProvider, LiteLLMServerConfig\n",
        "from src.retriever_agent.serper_rm import SerperRM\n",
        "from src.rag import RagAgent\n",
        "from src.dataclass import RagResponse, RagRequest\n",
        "\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "itwngXHoeB7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Argument and Rebuttal Layer"
      ],
      "metadata": {
        "id": "e00dTABpU1Y_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Currently this will be used only for the prime minister speech\n",
        "class ArgumentFormulator(dspy.Signature):\n",
        "    \"\"\"\n",
        "    You are in a debate tournament and tasked with generating arguments in support of or in opposition to a motion. You will be given the debate motion and your assigned position on that motion (whether you support or oppose it). Based on these inputs, generate up to 10 relevant arguments that address the motion from your assigned position. Each argument should be between 1-2 sentences long, 3 if absolutely necessary.\n",
        "    \"\"\"\n",
        "    motion: str = dspy.InputField(\n",
        "        desc=\"The debate motion being discussed\"\n",
        "    )\n",
        "    position: str = dspy.InputField(\n",
        "        desc=\"Our assigned position on the debate motion, which we will generate arguments from\"\n",
        "    )\n",
        "\n",
        "    arguments: List[str] = dspy.OutputField(\n",
        "        desc=\"List of generated arguments that address the debate mmoion from our assigned position\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "pSJ19mtJXxTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ideas not yet tried:\n",
        "\n",
        "# We could try to get the extractor to return two separate lists\n",
        "# - The most important arguments\n",
        "# - Th weakest arguments\n",
        "\n",
        "# We could also try to annotate the argument with its type, if it's a counterargument\n",
        "# to one of our side's arguments, for example, or if it's a standard argument\n",
        "# trope, for which we could draw on a database of successful responses to that\n",
        "# trope.\n",
        "\n",
        "class ArgumentExtractor(dspy.Signature):\n",
        "    \"\"\"\n",
        "    You are in a debate tournament and working on a response to the opposing side's speech. You will be given the debate motion, the opponent's assigned position, and the transcript of the opponent's speech. Based on this input, extract up to 6 of the most important or weakest arguments made by the opponent and return them in a list. Each argument should be condensed or summarized from claims made in the speech, and it should be between 1-2 sentences long, or at most 3 if absolutely necessary.\n",
        "    \"\"\"\n",
        "    opponent_speech: str = dspy.InputField(\n",
        "        desc=\"The transcript of the opposing team's speech\"\n",
        "    )\n",
        "    motion: str = dspy.InputField(\n",
        "        desc=\"The debate motion being discussed\"\n",
        "    )\n",
        "    position: str = dspy.InputField(\n",
        "        desc=\"The position of the opposing team in the debate\"\n",
        "    )\n",
        "\n",
        "\n",
        "    extracted_arguments: List[str] = dspy.OutputField(\n",
        "        desc=\"List of arguments extracted from the opponent's speech\"\n",
        "    )"
      ],
      "metadata": {
        "id": "PFsI5ikTrdoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shoud we ask the model to sort the refutations in order of strongest\n",
        "# to weakest?\n",
        "# Should we explicitly tell the model that short, pithy refutations\n",
        "# are okay, so it doesn't assume long-winded thoroughness is needed?\n",
        "class ArgumentRefuter(dspy.Signature):\n",
        "    \"\"\"\n",
        "    You are in a debate tournament and working on a response to the opposing side's speech. You are given the debate motion, the opponent's assigned position and a list of arguments extracted from the opponent's latest speech. Based on these inputs, refute the opponent's arguments, returning a list of at most 6 refutations. Each refutation may be anywhere between 1-8 sentences long and should identify the argument or arguments that you are refuting. You may also identify if contradictions between two or more of the opponent's arguments exist.\n",
        "    \"\"\"\n",
        "    extracted_arguments: List[str] = dspy.InputField(\n",
        "        desc=\"List of arguments extracted from the opponent's speech\"\n",
        "    )\n",
        "    motion: str = dspy.InputField(\n",
        "        desc=\"The debate motion being discussed\"\n",
        "    )\n",
        "    position: str = dspy.InputField(\n",
        "        desc=\"The position of the opposing team in the debate\"\n",
        "    )\n",
        "\n",
        "    refutations: List[str] = dspy.OutputField(\n",
        "        desc=\"List of refutations for the opponent's arguments\"\n",
        "    )"
      ],
      "metadata": {
        "id": "Vtk0FQeW-SWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Speech generators for each stage"
      ],
      "metadata": {
        "id": "cpvhXnFmU_Ri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Should we have different prompts for the first speech, the turns, and the concluding speech?\n",
        "Improve my prompts here"
      ],
      "metadata": {
        "id": "QdICNPiOK-Xm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OpeningSpeechGenerator(dspy.Signature):\n",
        "  \"\"\"\n",
        "  You are in a debate tournament and tasked with generating the prime minister speech, which is the\n",
        "  opening speech in the debate. Given the debate motion, your  position on this motion,\n",
        "  and a list of possible arguments you could use, generate\n",
        "  a well-structured, expressive debate speech. You are not limited to the\n",
        "  suggested arguments, and you do not need to draw from them; feel free to\n",
        "  generate your own.\n",
        "  \"\"\"\n",
        "  motion: str = dspy.InputField(\n",
        "      desc=\"The debate motion being discussed\"\n",
        "  )\n",
        "  position: str = dspy.InputField(\n",
        "      desc=\"Our assigned position on the debate motion\"\n",
        "  )\n",
        "  argument_suggestions: List[str] = dspy.InputField(\n",
        "      desc=\"List of possible arguments that may be helpful for our speech\"\n",
        "  )\n",
        "\n",
        "  speech: str = dspy.OutputField(\n",
        "      desc=\"The generated opening speech\"\n",
        "  )\n"
      ],
      "metadata": {
        "id": "TuJxo12CVK3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a speech in response to opponent's speech.\n",
        "# Should we include the original transcript at all? Or is including the refutations enough?\n",
        "# Should we include a list of all the arguments we've made, in case we want to add onto those arguments?\n",
        "# If not, should the speech be merely a response to the previous speaker's?\n",
        "# How do we refute the opponent's refutations?\n",
        "class ResponseGenerator(dspy.Signature):\n",
        "  \"\"\"\n",
        "  You are in a debate tournament and working on a response to the opposing side's speech.\n",
        "  You are given the debate motion, the opponent's assigned position on that motion,\n",
        "  and a list of possible refutations to the arguments made in the opponent's latest speech.\n",
        "  Based on these inputs, generate a well-structured debate speech responding to the opponent,\n",
        "  making sure to address the weaknesses in their argument.\n",
        "  \"\"\"\n",
        "  motion: str = dspy.InputField(\n",
        "      desc=\"The debate motion being discussed\"\n",
        "  )\n",
        "  position: str = dspy.InputField(\n",
        "      desc=\"The position of the opposing team in the debate\"\n",
        "  )\n",
        "  refutations: List[str] = dspy.InputField(\n",
        "      desc=\"List of refutations for the opponent's arguments\"\n",
        "  )\n",
        "\n",
        "  response: str = dspy.OutputField(\n",
        "      desc=\"The generated debate speech responding to the opponent\"\n",
        "  )"
      ],
      "metadata": {
        "id": "bypdau2lBeyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConcludingSpeechGenerator(dspy.Signature):\n",
        "  \"\"\"\n",
        "  You are in a debate tournament and working on your final response to the opposing side's speech.\n",
        "  You are given the debate motion, the opponent's assigned position on that motion,\n",
        "  and a list of possible refutations to the arguments made in the opponent's latest speech.\n",
        "  Based on these inputs, generate a well-structured debate speech responding to the opponent,\n",
        "  poking holes into their arguments, and wrapping up your side of the debate.\n",
        "  \"\"\"\n",
        "  motion: str = dspy.InputField(\n",
        "      desc=\"The debate motion being discussed\"\n",
        "  )\n",
        "  position: str = dspy.InputField(\n",
        "      desc=\"The position of the opposing team in the debate\"\n",
        "  )\n",
        "  refutations: List[str] = dspy.InputField(\n",
        "      desc=\"List of refutations for the opponent's arguments\"\n",
        "  )\n",
        "\n",
        "  response: str = dspy.OutputField(\n",
        "      desc=\"The generated debate speech responding to the opponent\"\n",
        "  )"
      ],
      "metadata": {
        "id": "A7-095P0fEfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments"
      ],
      "metadata": {
        "id": "3Fo908S9fx7C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if we improve on SOTA model performance\n",
        "\n",
        "And then check using only smaller models, since that is more realistic improvement"
      ],
      "metadata": {
        "id": "j0MG-P20lixH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "speech_generation_lm_config = LanguageModelProviderConfig(\n",
        "    provider=LanguageModelProvider.LANGUAGE_MODEL_PROVIDER_LITELLM_SERVER,\n",
        "    model_name=\"gpt-4.1\", # Reasoning for using this model for speech generation?\n",
        "    temperature=1.0,\n",
        "    max_tokens=10000, # Fix, how long should a speech be?\n",
        "    litellm_server_config=LiteLLMServerConfig(api_key=userdata.get(\"LITELLM_API_KEY\"), api_base=userdata.get(\"LITELLM_API_BASE\"))\n",
        ")\n",
        "speech_generation_lm = init_lm(speech_generation_lm_config)"
      ],
      "metadata": {
        "id": "FFZOVXthf1O2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "argument_formulation_lm_config = LanguageModelProviderConfig(\n",
        "    provider=LanguageModelProvider.LANGUAGE_MODEL_PROVIDER_LITELLM_SERVER,\n",
        "    model_name=\"gpt-4.1\", # Reasoning for using this model for argument formulation?\n",
        "    temperature=1.0,\n",
        "    max_tokens=10000, # Fix, how long should each argument be? No more than 3 sentences, which is how many tokens?\n",
        "    litellm_server_config=LiteLLMServerConfig(api_key=userdata.get(\"LITELLM_API_KEY\"), api_base=userdata.get(\"LITELLM_API_BASE\"))\n",
        ")\n",
        "argument_formulation_lm_config = init_lm(argument_formulation_lm_config)"
      ],
      "metadata": {
        "id": "KoUk5ZBDkkz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rebuttal_lm_config = LanguageModelProviderConfig(\n",
        "    provider=LanguageModelProvider.LANGUAGE_MODEL_PROVIDER_LITELLM_SERVER,\n",
        "    model_name=\"gpt-4.1\", # Reasoning for using this model for rebuttal formulation?\n",
        "    temperature=1.0,\n",
        "    max_tokens=10000, # Fix, how long should each rebuttal be? No more than 8 sentences, which is how many tokens?\n",
        "    litellm_server_config=LiteLLMServerConfig(api_key=userdata.get(\"LITELLM_API_KEY\"), api_base=userdata.get(\"LITELLM_API_BASE\"))\n",
        ")\n",
        "rebuttal_lm_config = init_lm(rebuttal_lm_config)"
      ],
      "metadata": {
        "id": "HrBp4H3WmdIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_initial_arguments = dspy.Predict(ArgumentFormulator) # Generate arguments given a motion and position\n",
        "extract_arguments = dspy.Predict(ArgumentExtractor) # Extract arguments from a transcript of the opponent's speech. Takes in opponent_speech, motion, position. Outputs extracted_arguments\n",
        "refute_arguments = dspy.Predict(ArgumentRefuter) # Respond to arguments extracted from the opponent's speech. Input: extracted_arguments, motion, position. Output: refutations\n",
        "\n",
        "generate_opening_speech = dspy.Predict(OpeningSpeechGenerator) # Generate the opening speech. Input: motion, position, argument_suggestions. Output: speech\n",
        "generate_response = dspy.Predict(ResponseGenerator) # Generate a response to the opponent's speech. Input: motion, position, refutations. Output: response\n",
        "generate_concluding_speech = dspy.Predict(ConcludingSpeechGenerator) # Generate a concluding speech. Same fields as in generate_response, just different instructions."
      ],
      "metadata": {
        "id": "yRpwSAiukRKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with dspy.context(lm=argument_formulation_lm):\n",
        "    initial_arguments = (await generate_initial_arguments.aforward(\n",
        "        motion=\n",
        "        position=\n",
        "    )).arguments"
      ],
      "metadata": {
        "id": "iYAWm9HRn7qV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}